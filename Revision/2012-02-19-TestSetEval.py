# -*- coding: utf-8 -*-
"""Testset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HUbs4abufx7r2OBsS_AKD7_5_-h_C1kO

# Whole Test Set

This file calculates accuracies for the test set with the prediction based on the quantity of labels for each filter.
- for all convolutional layers (method A)
- data from different runs
- no data for method B!

# Imports
"""


import logging
import numpy as np
import os
import pickle
from datetime import datetime, timedelta
from tqdm import tqdm
from warnings import warn, filterwarnings

"""# Input"""

network = "VGG16"
pickle_file = "/home/ws/ns1888/FilterLabel/codes/resultstest2/2021-02-03-1950-VGG16-E-vars"
pickle_file2 = "/home/ws/ns1888/FilterLabel/codes/resultstest2/2021-02-09-1405-VGG16-E-vars"
save_dir = "/home/ws/ns1888/FilterLabel/codes/resultstest4"
k = [i for i in range(1, 50)]
q1 = [1, 5]
q2 = [2, 3, 4, 10, 25]
q = q1+q2
q.sort()
classes = [i for i in range(1000)]  # snake(52:69),dogs(151:269), testset(1000)

# Test and debug
loglevel = "INFO"


# Preparations

now = datetime.utcnow()+timedelta(hours=1)
runid = now.strftime('%Y-%m-%d-%H%M')
if not save_dir:
    save_dir = os.getcwd()
save_vars = save_dir + "/" + runid + "-" + network + "-PH-vars"
save_vars_names = save_dir + "/" + runid + "-" + network + "-PH-vars-names"
save_vars_list = []
save_vars_info = save_vars + "-info.txt"
save_vars_results = save_vars + "-results.txt"
with open(save_vars_info, "a") as txt:
    txt.write("Vars Info PH\n----------\n\n")
    txt.write("This file gives an overview of the stored variables in pickle file "
            + save_vars + "\n")
    txt.write("To load them use e.g.:\n")
    txt.write("with open(\"" + save_vars_names + "\",\"rb\") as r:\n")
    txt.write("  var_names=pickle.load(r)\n")
    txt.write("with open(\"" + save_vars + "\",\"rb\") as p:\n")
    txt.write("  for var in var_names:\n")
    txt.write("    globals()[var]=pickle.load(p)\n\n")
    txt.write("stored variables are:\n")

# logging
logging.captureWarnings(True)
logger = logging.getLogger('logger1')
logger.setLevel(loglevel)
if logger.hasHandlers():
    logger.handlers.clear()
info_handler = logging.FileHandler(save_dir + "/" + runid + '-PH-info.log')
info_handler.setLevel(logging.INFO)
infoformat = logging.Formatter('%(asctime)s - %(message)s',
                               datefmt='%d.%m.%Y %H:%M:%S')
info_handler.setFormatter(infoformat)
logger.addHandler(info_handler)
stream_handler = logging.StreamHandler()
stream_handler.setLevel(logging.INFO)
stream_handler.setFormatter(infoformat)
logger.addHandler(stream_handler)
warnings_logger = logging.getLogger("py.warnings")
warnings_handler = logging.FileHandler(save_dir + "/" + runid + '-PH-warnings.log')
warnings_logger.addHandler(warnings_handler)
warnings_stream_handler = logging.StreamHandler()
warnings_logger.addHandler(warnings_stream_handler)
if logger.getEffectiveLevel() == 10:
    debug_handler = logging.FileHandler(savedir + "/" + runid + '-PH-debug.log')
    debug_handler.setLevel(logging.DEBUG)
    debugformat = logging.Formatter('%(asctime)s - %(levelname)s - %(funcName)s -'
                                    + ' %(message)s', datefmt='%d.%m.%Y %H:%M:%S')
    debug_handler.setFormatter(debugformat)
    logger.addHandler(debug_handler)
logger.propagate = False


"""## Hits@q and MRR"""

def compute_hitrates(z,z_hat,z_hat_per,images_files,q):
  '''
  This method calculates the accuracies for a class

  inputs:
  - z: network predictions
  - z_hat: approach predictions for q
  - z_hat_per: approach predictions for q%
  - images_files:dict with paths to the images
  - q: list with values for q

  returns:
  - acc: accuracies for different q
  - acc_per: accuracies for different q%

  remark: This function may cause a high number of warnings especially for
          high k
  '''
  logger.info("computing hitrates")
  k=[i for i in range(101)] #relaxation: take k most labels as prediction
  acc={}
  acc_per={}
  numimages=len(images_files)
  for el in q:
    acc[el]={}
    acc_per[el]={}
    for re in k:
      acc[el][re]=0
      acc_per[el][re]=0
  for imgnum,image in enumerate(images_files):
    for re in k:
      for el in q:
        correct=0
        correct_per=0
        for r in range(re+1):
          try:
            correct=correct+(z_hat[imgnum][el][r][0]==z[imgnum][0][0][0])
          except IndexError:
            #warn("only "+str(r)+" predictions for "+str(image)+" (q="+str(el)+")")
            break
        for r in range(re+1):
          try:
            correct_per=correct_per+(z_hat_per[imgnum][el][r][0]
                                     ==z[imgnum][0][0][0])
          except IndexError:
            #warn("only "+str(r)+" predictions for: "+str(image)+" (q%="+str(el)+")")
            break
        acc[el][re]=acc[el][re]+correct
        acc_per[el][re]=acc_per[el][re]+correct_per
  for el in q:
    for re in k:
      acc[el][re]=acc[el][re]/numimages
      acc_per[el][re]=acc_per[el][re]/numimages
  return acc,acc_per

def compute_MRR(z,z_hat,z_hat_per,images_files,q):
  '''
  This method calculates the accuracies for a class

  inputs:
  - z: network predictions
  - z_hat: approach predictions for q
  - z_hat_per: approach predictions for q%
  - images_files:dict with paths to the images
  - q: list with values for q

  returns:
  - mrr: mean reciprocal rank for different q
  - mrr_per: mean reciprocal rank for different q%
  '''
  logger.info("computing mrr")
  mrr={}
  mrr_per={}
  rr={}
  rr_per={}
  numimages=len(images_files)
  for el in q:
    rr[el] = {}
    rr_per[el] = {}
  pred_avail = False
  for el in q:
    for imgnum in range(numimages):
      #q
      for i in range(len(z_hat[imgnum][el])):
        if z_hat[imgnum][el][i][0] == z[imgnum][0][0][0]:
          rr[el][imgnum]=i+1
          pred_avail = True
          break
      if not pred_avail:
        rr[el][imgnum] = float("NaN")
      #q%
      pred_avail=False
      for i in range(len(z_hat_per[imgnum][el])):
        if z_hat_per[imgnum][el][i][0] == z[imgnum][0][0][0]:
          rr_per[el][imgnum]=i+1
          pred_avail = True
          break
      if not pred_avail:
        rr_per[el][imgnum] = float("NaN")

    mrr[el]=np.nanmedian(list(rr[el].values()))
    mrr_per[el]=np.nanmedian(list(rr_per[el].values()))

  return rr, rr_per, mrr, mrr_per

hit = {}
hit_per = {}
rank = {}
rank_per = {}
mrr = {}
mrr_per = {}
paths = {}

#vars for file2
hit_b = {}
hit_per_b = {}
rank_b = {}
rank_per_b = {}
mrr_b = {}
mrr_per_b = {}
paths_b = {}

for c, i in enumerate(tqdm(classes)):
    # load File
    num_vars = 0
    try:
        with open(pickle_file + str(i), "rb") as r:
            for el in ["y", "y_hat", "y_hat_ll", "y_hat_per", "y_hat_per_ll",
                       "class_images_dict"]:
                globals()[el] = pickle.load(r)
                num_vars = num_vars + 1
    except EOFError:
        warn("only " + num_vars + "/6 vars from pickle file")
    paths[i] = class_images_dict

    try:
        hit[i], hit_per[i] = compute_hitrates(y[i], y_hat[i], y_hat_per[i], class_images_dict[i], q1)
        rank[i], rank_per[i], mrr[i], mrr_per[i] = compute_MRR(y[i], y_hat[i], y_hat_per[i], class_images_dict[i], q1)

    except IndexError:
        warn("not able to compute hitrate for class " + str(i))

    # File2
    num_vars = 0
    try:
        with open(pickle_file2 + str(i), "rb") as r:
            for el in ["y", "y_hat", "y_hat_ll", "y_hat_per", "y_hat_per_ll",
                       "class_images_dict"]:
                globals()[el] = pickle.load(r)
                num_vars = num_vars + 1
    except EOFError:
        warn("only " + num_vars + "/6 vars from pickle file")
    paths_b[i] = class_images_dict

    try:
        hit_b[i], hit_per_b[i] = compute_hitrates(y[i], y_hat[i], y_hat_per[i], class_images_dict[i], q2)
        rank_b[i], rank_per_b[i], mrr_b[i], mrr_per_b[i] = compute_MRR(y[i], y_hat[i], y_hat_per[i], class_images_dict[i], q2)
    except IndexError:
        warn("not able to compute hitrate for class " + str(i))

    #merge vars
    hit[i].update(hit_b[i])
    hit_per[i].update(hit_per_b[i])
    rank[i].update(rank_b[i])
    rank_per[i].update(rank_per_b[i])
    mrr[i].update(mrr_b[i])
    mrr_per[i].update(mrr_per_b[i])

"""totalhit[q][k]: accuracies of the superset
- q: model parameter, model takes the q-most active filters of each layer for labelling (method A)
- k: model parameter, model takes the k labels with the highest counts for prediction
"""
logger.info("calculating hits@k and mrr over whole set")
total_img = 0
for i in classes:
    total_img = total_img + len(paths[i])

# q
totalhit = {}
totalmrr = {}
for el in q:
    totalhit[el] = {}
    totalmrr[el] = 0
    for re in k:
        totalhit[el][re] = 0
        for i in classes:
            totalhit[el][re] = totalhit[el][re] + hit[i][el][re] * len(paths[i])
    for i in classes:
        totalmrr[el]= totalmrr[el]+mrr[i][el] * len(paths[i])

# q%
totalhit_per = {}
totalmrr_per = {}
for el in q:
    totalhit_per[el] = {}
    totalmrr_per[el] = 0
    for re in k:
        totalhit_per[el][re] = 0
        for i in classes:
            totalhit_per[el][re] = totalhit_per[el][re] + hit_per[i][el][re] * len(paths[i])
    for i in classes:
        totalmrr_per[el]= totalmrr_per[el]+mrr_per[i][el] * len(paths[i])

# scale
for el in q:
    totalmrr[el] = totalmrr[el] / total_img
    totalmrr_per[el] = totalmrr_per[el] / total_img
    for re in k:
        totalhit[el][re] = totalhit[el][re] / total_img
        totalhit_per[el][re] = totalhit_per[el][re] / total_img

# save session vars as pickle file
save_vars_list=["hit","hit_per","rank","rank_per","mrr","mrr_per","paths","totalhit", "totalhit_per", "totalmrr",
                "totalmrr_per"]
with open(save_vars, "ab") as p:
    for el in save_vars_list:
        pickle.dump(globals()[el], p)
with open(save_vars_info, "a") as txt:
    for el in save_vars_list:
        txt.write("- " + el + "\n")
with open(save_vars_names, 'ab') as p:
    pickle.dump(save_vars_list, p)
logger.info("---run successful---")