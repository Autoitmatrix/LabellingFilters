# -*- coding: utf-8 -*-
"""2020-12-22-Step1CA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cVAdRZVuDdYjtzwS44JWVC4myAK34e-6
"""

# -*- coding: utf-8 -*-
"""Step1 Collecting Activations"""

# Imports
import logging
import numpy as np
import os
import pandas as pd
import pickle
# import tensorflow as tf
from datetime import datetime, timedelta
from numba import jit
from sklearn.preprocessing import MinMaxScaler
from tensorflow import keras
# from tensorflow.keras import preprocessing
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tqdm import tqdm
from warnings import warn

"""## Inputs"""

# Input
network = "VGG16"
images_dir = "/home/ws/ns1888/FilterLabel/ILSVRC2012_img_train"
save_dir = "/home/ws/ns1888/FilterLabel/codes/resultstest4"
seed_rn = 123
split = 0.2  # data taken for validation

# debugging and testing input
loglevel = "INFO"
res_classes = None  # restrict number of classes

"""## Preparations"""

# create ID and paths for files
now = datetime.utcnow() + timedelta(hours=1)
runid = now.strftime("%Y-%m-%d-%H%M")
if not save_dir:
    save_dir = os.getcwd()
save_vars = save_dir + "/" + runid + "-" + network + "-CA-vars"
save_vars_names = save_dir + "/" + runid + "-" + network + "-CA-vars-names"
save_vars_list = []
save_vars_info = save_vars + "-info.txt"

# create info file
with open(save_vars_info, "a") as q:
    q.write("Vars Info CA\n----------\n\n")
    q.write("This file gives an overview of the stored variables in pickle file"
            " "+ save_vars + "\n")
    q.write("To load them use e.g.:\n")
    q.write("with open(\"" + save_vars_names + "\",\"rb\") as r:\n")
    q.write("  var_names=pickle.load(r)\n")
    q.write("with open(\"" + save_vars + "\",\"rb\") as p:\n")
    q.write("  for var in var_names:\n")
    q.write("    globals()[var]=pickle.load(p)\n\n")
    q.write("stored variables are:\n")

# logging
logging.captureWarnings(True)
logger = logging.getLogger('logger1')
logger.setLevel(loglevel)
if (logger.hasHandlers()):
    logger.handlers.clear()
info_handler = logging.FileHandler(save_dir + "/" + runid + '-CA-info.log')
info_handler.setLevel(logging.INFO)
infoformat = logging.Formatter('%(asctime)s - %(message)s', 
                               datefmt='%d.%m.%Y %H:%M:%S')
info_handler.setFormatter(infoformat)
logger.addHandler(info_handler)
stream_handler = logging.StreamHandler()
stream_handler.setLevel(logging.INFO)
stream_handler.setFormatter(infoformat)
logger.addHandler(stream_handler)
warnings_logger = logging.getLogger("py.warnings")
warnings_handler = logging.FileHandler(save_dir + "/" + runid + 
                                       '-CA-warnings.log')
warnings_logger.addHandler(warnings_handler)
warnings_stream_handler = logging.StreamHandler()
warnings_logger.addHandler(warnings_stream_handler)
if logger.getEffectiveLevel() == 10:
    debug_handler = logging.FileHandler(save_dir + "/" + runid + 
                                        '-CA-debug.log')
    debug_handler.setLevel(logging.DEBUG)
    debugformat = logging.Formatter('%(asctime)s - %(levelname)s - %(funcName)s'
    ' - %(message)s',
                                    datefmt='%d.%m.%Y %H:%M:%S')
    debug_handler.setFormatter(debugformat)
    logger.addHandler(debug_handler)
logger.propagate = False

# images
if "ception" in network:  # Inception networks have other image resolution
    img_rows = 299
    img_cols = 299
else:
    img_rows = 224
    img_cols = 224

# network and preprocessing function
network_mod = {"VGG16": "vgg16", 
               "VGG19": "vgg19", 
               "InceptionV3": "inception_v3", 
               "ResNet50": "resnet",
               "ResNet101": "resnet", 
               "ResNet152": "resnet", 
               "ResNet50V2": "resnet_v2", 
               "ResNet101V2": "resnet_v2",
               "ResNet152V2": "resnet_v2", 
               "InceptionResNetV2": "inception_resnet_v2"}
logger.info("importing network:")
cnn = getattr(keras.applications, network)()
network_module = getattr(keras.applications, network_mod[network])
preprocess_input = getattr(network_module, "preprocess_input")
layers = [] # interesting layers (conv), omitting pooling layer and fc
for i, layer in enumerate(cnn.layers):
    if 'conv' not in layer.name:
        continue
    layers.append(i)
outputs = [cnn.layers[i].output for i in layers]
outputs_model = keras.Model([cnn.input], outputs)

"""## functions"""

def preprocess_images_from_dir(i_dir, preprocess_fun, i_rows=224, i_cols=224, 
                               seed_rn="123", val_split=0.2, res=None):
    """
    prepares the images in images_dir as input for the networks.

    input:
    - i_dir: path to the directory with image data, the folders in it should be 
             named after the image class of the images they contain.
    - preprocess_fun: preprocessing function
    - i_rows: height of images
    - i_cols: width of images
    - seed_rn: random seed
    - val_split: fraction used for test set, 1-split is used for train set
    - res: restriction of the number of classes (for test purposes)

    returns:
    - images_train: train set
    - images_test: test set

    remark:
    uses tensorflow.keras as keras and ImageDataGenerator from 
    tensorflow.keras.preprocessing.image
    """
    # prepare datagenerator
    datagen = ImageDataGenerator(preprocessing_function=preprocess_fun, 
                                 validation_split=val_split)
    # create dataframe with infos for imagespaths and shuffle it
    folders = os.listdir(i_dir)
    numfolders = len(folders)
    if res:
        numfolders = res
    files = []
    target = []
    for folder in folders[0:numfolders]:
        imagepaths = os.listdir(i_dir + "/" + folder)
        for imagepath in imagepaths:
            files.append(i_dir + "/" + folder + "/" + imagepath)
            target.append(folder)
    df_images = pd.DataFrame({"imagepath": files, "wnid": target})
    df_images = df_images.sample(frac=1, 
                                 random_state=seed_rn).reset_index(drop=True)  
    # collect train images
    logger.info("Preparing Train Set")
    images_train = datagen.flow_from_dataframe(df_images, x_col='imagepath', 
                                               y_col='wnid', batch_size=1,
                                               target_size=(i_rows, i_cols), 
                                               shuffle=False, seed=seed_rn,
                                               subset="training")
    # collect test images
    logger.info("Preparing Test Set")
    images_test = datagen.flow_from_dataframe(df_images, x_col='imagepath', 
                                              y_col='wnid', batch_size=1,
                                              target_size=(i_rows, i_cols), 
                                              shuffle=False, seed=seed_rn,
                                              subset="validation")
    logger.info("images ready")
    return images_train, images_test


def get_activations(imgs, model, aggregate="mean", method="classfirst"):
    """
    computes the average or max activations of prep_images according to model

    input:
    - imgs (list): image data as np.arrays
    - model (keras.model): keras model to obtain the activations
    - aggregate (String): either "max" or "mean" determines the aggregation of
                          the activations for the images of each class
    - method (String): either "fmfirst" or "classfirst"

    returns:
    - filter_act: featuremap values (activations of filters)
    """
    if aggregate not in ["max", "mean"]:
        warn(str(method)+" not allowed for allocation of activations. "
        "Aggregation is set to \"mean\"")
        aggregate="mean"

    if method=="classfirst":
      filter_act=classfirst(imgs, outputs_model, aggregate)

    if method=="fmfirst":
      filter_act=fmfirst(imgs, outputs_model, aggregate)

    return filter_act


def scale_out(a):
    '''
    scales the activations of each layer to a range of [0,1]

    input:
    - a: raw activations

    return:
    - b: scaled activations
    '''
    b = a.copy()
    scaler = MinMaxScaler(copy=False)  # inplace operation
    for layer in b:
        layer = layer.reshape(-1, 1)
        scaler.fit_transform(layer)
    return b
@jit(nopython=True)
def classfirst(imgs, outputs_model, aggregate):
    """

    """
    for step,img in enumerate(imgs):
        raw_acts=outputs_model.predict(img) 
        out_pic_new = scale_out(raw_acts)
        if step==0:
            prep_act = out_pic_new.copy()
        else:
            for count, layer in enumerate(prep_act):
                if aggregate == "max":
                  prep_act[count] = np.maximum(out_pic_new[count], 
                                               out_pic_old[count])
                else:  # aggregate == "mean":
                    prep_act[count] = (out_pic_new[count] * (1 / (step + 1)) + 
                             out_pic_old[count] * (step / (step + 1)))                    
        out_pic_old = prep_act.copy()
    fm_val = featuremap_values(prep_act,aggregate)
    return fm_val

def fmfirst(imgs, outputs_model, aggregate):
    """
  
    """
    for step,img in enumerate(imgs):
        raw_acts=outputs_model.predict(img) 
        out_pic_new = scale_out(raw_acts)
        fm_val_new = featuremap_values(out_pic_new,aggregate)
        if step==0:
            prep_act = fm_val_new.copy()
        else:
            for count, layer in enumerate(prep_act):
                if aggregate == "max":
                  prep_act[count] = np.maximum(fm_val_new[count], fm_val[count])
                else:  # aggregate == "mean":
                    prep_act[count] = (fm_val_new[count] * (1 / (step + 1)) + 
                             fm_val[count] * (step / (step + 1)))                    
        fm_val = prep_act.copy()
    return fm_val

def featuremap_values(acts,aggregate):
    '''
    averages the activations of each feature map to get one single number for 
    each feature map

    input:
    - acts: activations

    return:
    - featuremap_val: feature map values (averaged activations for each feature
      map)
    '''
    featuremap_val = [[] for x in range(len(acts))]
    for i, layer in enumerate(acts):
        if aggregate=="max":
          featuremap_val[i] = layer.max(axis=(0,1,2))
        else:  # aggregate=="mean"
          featuremap_val[i] = layer.mean(axis=(0,1,2))

    return featuremap_val

""" #Execution """

logger.info("Selected network is " + network)

# preprocess images
train, test = preprocess_images_from_dir(images_dir, preprocess_input, img_rows,
                                         img_cols,  seed_rn, split, res_classes)

# extract class labels
labels_id_inv = train.class_indices  # get dict with mapping of labels
labels_id = {y: x for x, y in labels_id_inv.items()}  # invert dict
with open(save_vars, "wb") as p:
    pickle.dump(train.filenames, p)
    pickle.dump(test.filenames, p)
    pickle.dump(labels_id, p)
save_vars_list.append("train_filenames")
save_vars_list.append("test_filenames")
save_vars_list.append("labels_id")
with open(save_vars_info, "a") as q:
    q.write("- train_filenames: list wiht filenames of train set\n")
    q.write("- test_filenames: list with filenames of test set\n")
    q.write("- labels_id: dictionary of numerique train labels and "
            "corresponding folder name (class)\n")
logger.info("class labels added to " + save_vars)
num_classes = len(labels_id)
outputs = [cnn.layers[i].output for i in layers]
outputs_model = keras.Model([cnn.input], outputs)

fil_act = []
for i in tqdm(range(num_classes)):
    logger.info("class: " + str(labels_id[i]) + " (labelnum:" + str(i) + ")")
    class_images = [train[j][0] for j in range(train.n) if train.labels[j] == i]
    out = get_activations(class_images, outputs_model,aggregate, method)
    fil_act.append(out)

with open(save_vars, "ab") as p:
    pickle.dump(fil_act, p)
save_vars_list.append("fil_act")
with open(save_vars_names, "wb") as r:
    pickle.dump(save_vars_list, r)
with open(save_vars_info, "a") as q:
    q.write("- fil_act: activations before threshold\n")
logger.info("activations added to " + save_vars)
logger.info("---------------Run succesful-----------------")